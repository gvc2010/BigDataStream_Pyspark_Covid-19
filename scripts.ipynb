{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Dados do Covid-19 no Brasil**\n",
        "Fiz o upload do arquivo ***covid.csv*** no meu github [Perfil do GitHub](https://github.com/gvc2010) /big_data_stream\n",
        "\n",
        "\n",
        "Ele se localizava em ***cd ~/BigDataStream/apache_spark/Semana_4.***\n",
        "Tive problemas de permissão para realizar a atividade na VM, conforme dito no ***\"Fale com a gente\"***\n",
        "\n",
        "Estarei realizando a ATP com o **Google Colab**\n",
        "\n",
        "**Aluno: Guilherme Venturini de Castro**"
      ],
      "metadata": {
        "id": "9hqvoMwJUjgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# Nome do campo Descrição**\n",
        "\n",
        "3 dataNascimento Data de nascimento\n",
        "\n",
        "4 sintomas Sintomas do paciente\n",
        "\n",
        "5 profissionalSaude Relacionado a profissional de saúde\n",
        "\n",
        "6 cbo Ocupação\n",
        "\n",
        "7 condicoes Condições do paciente\n",
        "\n",
        "8 estadoTeste Estado do teste\n",
        "\n",
        "9 dataTeste Data do teste\n",
        "\n",
        "10 tipoTeste Tipo de teste realizado\n",
        "\n",
        "11 resultadoTeste Resultado do teste\n",
        "\n",
        "12 paisOrigem País de origem do paciente\n",
        "\n",
        "13 sexo Sexo do paciente\n",
        "\n",
        "14 bairro Bairro do paciente\n",
        "\n",
        "15 estado Estado do paciente\n",
        "\n",
        "16 estadoIBGE Estado do paciente IBGE\n",
        "\n",
        "17municipio Município do paciente\n",
        "\n",
        "18municipioIBGE Município do paciente IBGE\n",
        "\n",
        "19 cep CEP\n",
        "\n",
        "20 origem Origem do paciente\n",
        "\n",
        "21 cnes Código da unidade de saúde\n",
        "\n",
        "22 estadoNotificacao Estado da notificação\n",
        "\n",
        "23 estadoNotificacaoIBGE Estado da notificação IBGE\n",
        "\n",
        "24municipioNotificacao Município da notificação\n",
        "\n",
        "25municipioNotificacaoIBGE Município da notificação IBGE\n",
        "\n",
        "26 numeroNotificacao Número da notificação\n",
        "\n",
        "27 excluido ID de exclusão\n",
        "\n",
        "28 validado Local de validação\n",
        "\n",
        "29 idade Idade do paciente\n",
        "\n",
        "30 dataEncerramento Data do encerramento da avaliação do paciente\n",
        "\n",
        "31 evolucaoCaso Evolução do caso do paciente\n",
        "\n",
        "32 classificacaoFinal Avaliação final do caso\n"
      ],
      "metadata": {
        "id": "Olvr-lZDVN2v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGpatFJu3Mxb",
        "outputId": "c188bc31-af66-4bd2-aff9-c048c8557f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=e17b61abc93d0101d1c604328981163a9bfe023ddb923be8c799a8cae6de40d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2024-06-24 22:21:20\n",
            "-------------------------------------------\n",
            "(('Masculino', 'Negativo'), 5)\n",
            "(('Feminino', 'Positivo'), 1)\n",
            "(('Masculino', 'Positivo'), 2)\n",
            "(('Feminino', 'Negativo'), 2)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-06-24 22:21:20\n",
            "-------------------------------------------\n",
            "(' Tosse', 2)\n",
            "('Febre', 1)\n",
            "(' Dor de Garganta', 1)\n",
            "('Outros', 1)\n",
            "('Dispneia', 1)\n",
            "(' Febre', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-06-24 22:21:20\n",
            "-------------------------------------------\n",
            "37\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-06-24 22:21:20\n",
            "-------------------------------------------\n",
            "('Nova Olímpia', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-06-24 22:21:20\n",
            "-------------------------------------------\n",
            "('Monday', 3)\n",
            "('Thursday', 2)\n",
            "('Tuesday', 2)\n",
            "('Friday', 1)\n",
            "('Sunday', 1)\n",
            "('Wednesday', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-06-24 22:21:30\n",
            "-------------------------------------------\n",
            "(('Masculino', 'Negativo'), 7)\n",
            "(('Feminino', 'Positivo'), 4)\n",
            "(('Masculino', nan), 1)\n",
            "(('Masculino', 'Positivo'), 3)\n",
            "(('Feminino', 'Negativo'), 4)\n",
            "(('Feminino', nan), 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2024-06-24 22:21:30\n",
            "-------------------------------------------\n",
            "(' Tosse', 5)\n",
            "('Febre', 4)\n",
            "(' Outros', 3)\n",
            "(' Dor de Garganta', 2)\n",
            "(' Febre', 2)\n",
            "('Outros', 1)\n",
            "('Tosse', 1)\n",
            "('Dispneia', 1)\n",
            "(' Dispneia', 1)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Instalar as dependências no Google Colab\n",
        "!pip install pyspark\n",
        "\n",
        "# Configurar o PySpark no Google Colab\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "ssc = StreamingContext(sc, 10)  # Intervalo de 10 segundos para o StreamingContext\n",
        "ssc.checkpoint(\"/tmp/checkpoint\")\n",
        "\n",
        "# Função para simular o envio de dados de um arquivo CSV para o DStream\n",
        "def simulate_stream(ssc, data):\n",
        "    rdd_queue = []\n",
        "    for i in range(0, len(data), 10):  # Dividindo os dados em pedaços de 10 linhas\n",
        "        rdd_queue += [ssc.sparkContext.parallelize(data[i:i + 10])]\n",
        "        time.sleep(0.1)  # Atraso de 100ms entre cada lote de dados\n",
        "    return rdd_queue\n",
        "\n",
        "# Carregar o CSV\n",
        "df = pd.read_csv('covid.csv', sep=';')\n",
        "data = df.to_numpy().tolist()  # Converter DataFrame para lista de listas\n",
        "\n",
        "rdd_queue = simulate_stream(ssc, data)\n",
        "input_stream = ssc.queueStream(rdd_queue)\n",
        "\n",
        "# Função para obter o dia da semana a partir da data\n",
        "import datetime\n",
        "\n",
        "def get_day_of_week(date_str):\n",
        "    date = datetime.datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "    return date.strftime('%A')\n",
        "\n",
        "# 1. Quantidade de pacientes positivos para coronavírus no último minuto e atualização a cada 30 segundos\n",
        "positive_cases = input_stream.filter(lambda line: line[11] == 'Positivo')\\\n",
        "                             .countByWindow(60, 30)\n",
        "positive_cases.pprint()\n",
        "\n",
        "# 2. Quantidade de pacientes de acordo com o sexo e o resultado do teste nos últimos 50 segundos e atualização a cada 20 segundos\n",
        "sex_test_result = input_stream.map(lambda line: ((line[13], line[11]), 1))\\\n",
        "                              .reduceByKeyAndWindow(lambda a, b: a + b, 50, 20)\n",
        "sex_test_result.pprint()\n",
        "\n",
        "# 3. Sintomas mais comuns para casos positivos para coronavírus no último minuto e atualização a cada 30 segundos\n",
        "common_symptoms = input_stream.filter(lambda line: line[11] == 'Positivo')\\\n",
        "                              .flatMap(lambda line: line[4].split(','))\\\n",
        "                              .map(lambda symptom: (symptom, 1))\\\n",
        "                              .reduceByKeyAndWindow(lambda a, b: a + b, 60, 30)\\\n",
        "                              .transform(lambda rdd: rdd.sortBy(lambda x: -x[1]))\n",
        "common_symptoms.pprint()\n",
        "\n",
        "# 4. Quantidade de casos positivos no Paraná nos últimos 40 segundos e atualização a cada 20 segundos\n",
        "positive_cases_pr = input_stream.filter(lambda line: line[15] == 'PARANÁ' and line[11] == 'Positivo')\\\n",
        "                                .countByWindow(40, 20)\n",
        "positive_cases_pr.pprint()\n",
        "\n",
        "# 5. Idade das mulheres positivas para Covid-19\n",
        "female_positive_ages = input_stream.filter(lambda line: line[13] == 'Feminino' and line[11] == 'Positivo')\\\n",
        "                                   .map(lambda line: int(line[29]))  # Convertendo idade para inteiro\n",
        "female_positive_ages.pprint()\n",
        "\n",
        "# 6. Município do Paraná com a maior quantidade de mulheres positivadas para Covid-19 no último minuto e atualização a cada 20 segundos\n",
        "positive_females_pr = input_stream.filter(lambda line: line[15] == 'PARANÁ' and line[13] == 'Feminino' and line[11] == 'Positivo')\\\n",
        "                                  .map(lambda line: (line[17], 1))\\\n",
        "                                  .reduceByKeyAndWindow(lambda a, b: a + b, 60, 20)\\\n",
        "                                  .transform(lambda rdd: rdd.sortBy(lambda x: -x[1]))\n",
        "positive_females_pr.pprint()\n",
        "\n",
        "# 7. Dia da semana com a maior quantidade de testes realizados nos últimos dois minutos e atualização a cada 40 segundos\n",
        "day_of_week_tests = input_stream.map(lambda line: (get_day_of_week(line[1]), 1))\\\n",
        "                                .reduceByKeyAndWindow(lambda a, b: a + b, 120, 40)\\\n",
        "                                .transform(lambda rdd: rdd.sortBy(lambda x: -x[1]))\n",
        "day_of_week_tests.pprint()\n",
        "\n",
        "# Iniciar o streaming e aguardar a conclusão\n",
        "ssc.start()\n",
        "ssc.awaitTerminationOrTimeout(120)\n",
        "ssc.stop(stopSparkContext=True, stopGraceFully=False)"
      ]
    }
  ]
}
